{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI_project3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMIPhbFvkBr7",
        "colab_type": "text"
      },
      "source": [
        "#Hossein Entezari Zarch\n",
        "##810196419\n",
        "#Artifical Intelligence\n",
        "##Project3\n",
        "Spring-2020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4zULG3MkSy2",
        "colab_type": "text"
      },
      "source": [
        "In the following part, The need packages are imported to be called in subsequent parts of the code.\n",
        "CSV: CSV package is used in order to read from data and store it in python arrays.\n",
        "NLTK: NLTK package is used to preprocess and clean text before it is put in algotithm for further works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVo6jowej5zR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "d3ac62bb-8773-42c2-ef6d-6d835073948f"
      },
      "source": [
        "import csv\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from math import log\n",
        "import random\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Obu3lYlDlKcp",
        "colab_type": "text"
      },
      "source": [
        "The following function was defined in order to read the data from the file and store it in 2D array and then utlizes clean_text function to clean the text in each element of the array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikmyn8PblICU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_csv(file_name):\n",
        "    fields = []\n",
        "    rows = []\n",
        "    with open(file_name, 'r') as csv_file:\n",
        "        csv_reader = csv.reader(csv_file)\n",
        "        fields = next(csv_reader)\n",
        "        for row in csv_reader:\n",
        "            rows.append(row)\n",
        "        for row in rows:\n",
        "            row[4] = clean_text(row[4])\n",
        "            row[6] = clean_text(row[6])\n",
        "    return fields, rows\n",
        "\n",
        "def read_csv_test(file_name):\n",
        "    fields = []\n",
        "    rows = []\n",
        "    with open(file_name, 'r') as csv_file:\n",
        "        csv_reader = csv.reader(csv_file)\n",
        "        fields = next(csv_reader)\n",
        "        for row in csv_reader:\n",
        "            rows.append(row)\n",
        "        for row in rows:\n",
        "            row[1] = clean_text(row[1])\n",
        "            row[4] = clean_text(row[4])\n",
        "    return fields, rows"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud1oRO5Gmt-1",
        "colab_type": "text"
      },
      "source": [
        "This function uses different methods available in NLTK package to tokenize, normalize and other preprocessing proccedures needed to be done on text.\n",
        "Steps: At first, we tokenize the text with word_tokenize function available in nltk package gives us an array of wors in the input sentence, Then with calling isalpha() function on tokenized sentencs items we would get True if the word is an alphabetic otherwise, if the word is a punctuation of something elese we would get False on the output of the function, so in this line of code we remove the punctuations from the text, Then with calling lower() function we make all of the words to lowercase further, We know we need to remove the stopwords from the text for the proccdures we would perform on them so, we remove all of the words that are not in nltk.corpus.stopwords.words(\"english\") which includes all of the english stopwords, At last we need to normalize the text the details of the process is available below:\n",
        "In general there are two methods available to normalize the text \"Stemming\" and \"Lemmatization\".\n",
        "\n",
        "Stemming: This method tries to normalized a word by reducing the word itself stem for instance, \"automate\", \"automatic\", \"automation\", \"automations\" will be reduced to \"automat\", it is proper to use when the stem of all forms of a word are the same, unless, lemmitization would be a better chioce.\n",
        "\n",
        "Lemmatization: This method uses the wordnet database to perform the lemmatization.\n",
        "\n",
        "Stemming VS Lemmatization: I think the choose of lemmatization is more convenient as this algorithm does not remove any important part of information we have about meaning of the word, on the other hand the Stemming algorithm reduce any word to its stem.\n",
        "\n",
        "Need of lowercase the words: I think our algorithm for text processing is not dependant on the position of the word in the sentence, and most of the uppercase words represent the sentnece begins with that word which is not an crucial detail in out algorithm so, it would be better for us to lowercase the text to make similar words completely in an identical form.\n",
        "\n",
        "When we experimented the result on the Stemming method instead of the Lemmatization method we observed that the results just changed a little that seems because of randomness and shuffling process in oversampling step and we didn't see any progress or detoriate in the performance of the algorithm.\n",
        "\n",
        "Also we experimented without lowercase the text and we didn't see any meaningful change and at one experiment the total accracy decreased by 2 percent just."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZT8uzecCmrgU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "    tokens = [word for word in tokens if not word in nltk.corpus.stopwords.words(\"english\")]\n",
        "    lemma = nltk.WordNetLemmatizer()\n",
        "    tokens = [lemma.lemmatize(word, pos=\"v\") for word in tokens]\n",
        "    tokens = [lemma.lemmatize(word, pos=\"n\") for word in tokens]\n",
        "    # porter = PorterStemmer()\n",
        "    # for i in range(len(tokens)):\n",
        "    #     tokens[i] = porter.stem(tokens[i])\n",
        "    return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-XB-6HpHB7l",
        "colab_type": "text"
      },
      "source": [
        "This function is implemented to seperate the validation data from training_data, the function iterates over the data rows and put 80 percent of them in the train_data and the 20 percent others in valid_data also it seperates different classes of data equally, also our proccdure was in a way that keeps the ratio of different classes of data items we have unchanged in order to be sure that the seperating the data doesn't cause any problem in having enough data of different classes both in training set and validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgZ6RducG9nb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seperate_data(data):\n",
        "    train_data = []\n",
        "    valid_data = []\n",
        "    dictNum = {}\n",
        "    for row in data:\n",
        "        if not row[2] in dictNum:\n",
        "            dictNum[row[2]] = [1]\n",
        "        else:\n",
        "            dictNum[row[2]][0] += 1\n",
        "    for row in data:\n",
        "        if len(dictNum[row[2]]) < 2:\n",
        "            dictNum[row[2]].append(1)\n",
        "            train_data.append(row)\n",
        "        elif dictNum[row[2]][1] < dictNum[row[2]][0] *(8/10):\n",
        "            dictNum[row[2]][1] += 1\n",
        "            train_data.append(row)\n",
        "        else:\n",
        "            valid_data.append(row)\n",
        "    return train_data, valid_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocOjJ9HyH4Uy",
        "colab_type": "text"
      },
      "source": [
        "The following function, samples from the data with classes that their count is less than others and copy the sampled row in data then after perform a shuffling step it would return the final dataset.\n",
        "\n",
        "This solution helps our algorithm to see data from different classes equally and not be biased on data given from one or two class with more amount of data in training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIf1yk0uHyTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def over_sample(data):\n",
        "    numClasses = [0, 0, 0]\n",
        "    for row in data:\n",
        "        if row[2] == 'BUSINESS':\n",
        "            numClasses[0] += 1\n",
        "        elif row[2] == 'TRAVEL':\n",
        "            numClasses[1] += 1\n",
        "        else:\n",
        "            numClasses[2] += 1\n",
        "    \n",
        "    maxClass = max(numClasses)\n",
        "    diff = maxClass - numClasses[0]\n",
        "    newData = []\n",
        "    while len(newData) < diff:\n",
        "        s = random.choice(data)\n",
        "        if s[2] == 'BUSINESS':\n",
        "            newData.append(s)\n",
        "    for row in newData:\n",
        "        data.append(row)\n",
        "    \n",
        "    diff = maxClass - numClasses[1]\n",
        "    newData = []\n",
        "    while len(newData) < diff:\n",
        "        s = random.choice(data)\n",
        "        if s[2] == 'TRAVEL':\n",
        "            newData.append(s)\n",
        "    for row in newData:\n",
        "        data.append(row)\n",
        "    \n",
        "    diff = maxClass - numClasses[2]\n",
        "    newData = []\n",
        "    while len(newData) < diff:\n",
        "        s = random.choice(data)\n",
        "        if s[2] == 'STYLE & BEAUTY':\n",
        "            newData.append(s)\n",
        "    for row in newData:\n",
        "        data.append(row)\n",
        "    random.shuffle(data)\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVlFHyUTqBPl",
        "colab_type": "text"
      },
      "source": [
        "Confusion Matrix is a performance measure for machine learning classification problems which generally includes num of true labled cases on each case and also num of false labled cases on each case depending on the false label the machine put on it.\n",
        "In the following function you see how machine labels are distributed between classes of data.\n",
        "and the result of the function on our model is:\n",
        "<table>\n",
        "    <tr>\n",
        "        <th>actual label/predicted label</th>\n",
        "        <th>BUSINESS</th>\n",
        "        <th>TRAVEL</th>\n",
        "        <th>STYLE & BEAUTY</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <th>BUSINESS</th>\n",
        "        <td>1598</td>\n",
        "        <td>119</td>\n",
        "        <td>61</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <th>TRAVEL</th>\n",
        "        <td>307</td>\n",
        "        <td>1355</td>\n",
        "        <td>116</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <th>STYLE & BEAUTY</th>\n",
        "        <td>185</td>\n",
        "        <td>150</td>\n",
        "        <td>1443</td>\n",
        "    </tr>\n",
        "<table>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnjgpkYir9eO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def confusion_matrix(truth_labels):\n",
        "    ans = {}\n",
        "    for row in truth_labels:\n",
        "        if not row[0] in ans:\n",
        "            ans[row[0]] = {}\n",
        "        else:\n",
        "            if not row[1] in ans[row[0]]:\n",
        "                ans[row[0]][row[1]] = 1\n",
        "            else:\n",
        "                ans[row[0]][row[1]] += 1\n",
        "    return ans"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYnLFTD_bVYy",
        "colab_type": "text"
      },
      "source": [
        "Bayesian Rule : P(H|E)=P(E|H).P(H)/P(E)\n",
        "The formulat above is the bayesian theorem, we are to get some detail on its parts.\n",
        "-p(E): this term is the probability of the evidence occurance which is called  prior distribution, meaning that it represents our beliefs about truth of the parameters(here words we have in a short description).\n",
        "-p(H|E): This part is called the posterior probability, more precisely it is the probabitliy of hypothesis given the observed evidence.\n",
        "-p(E|H): This term is the probability of occuring E with given H and also it is named the likelihood, it indicates the compatibility of our evidence by the given hypothesis.\n",
        "-P(E): This term is called evidence and it is the probability of occuring the evidence we have."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b_smL39iwuT",
        "colab_type": "text"
      },
      "source": [
        "Proccedure of labeling: In our algorithm the main theorem we utilize is the bayesian theorem which makes it available for us to calculate p(c|X) when c represents a class and X represents the words in the sentence we are to label, the method is that to label a sentence containing words X, we calculate the p(c|X) for all available classes c and then label the c on sentence with the biggest value of p(c|X), according to the calculation approach we follow the formula as p(c|X) = p(X|c).p(c)/p(X) and as we know that, p(X) does not depend on c, so its calculation doesn't effect the comparison of p(c|X) values among different classes so we would omit this term from the formula, and also due to the situation of the problem, we know that, P(X) = p(x1).p(x2). ... as the p(x)s are independant to each other. so the final formulat we would use to compare between different classes on a sentence would be:\n",
        "p(c|X) = p(x1|c).p(x2|c). ... .p(xn|c).p(c)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80c6GXTJkmPF",
        "colab_type": "text"
      },
      "source": [
        "Calculate the value of p(xi|c):\n",
        "-In our impelmentation, the value of p(xi|c) is the num of times we see the word xi in sentences with class c divided into number of all of words we have in sentences with class c.\n",
        "-So, at learning step in train function, we store a triple array for each word we see in a dicionary that the first item stores num of times we saw word in the first class sentence and the second one and third one so on.\n",
        "-The numWords triple array stores num of all words we see in each class sentences.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woU7ClbulgdV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model:\n",
        "    def __init__(self, file_name):\n",
        "        self.data_fields, data = read_csv(file_name)\n",
        "        data = over_sample(data)\n",
        "        self.train_data, self.valid_data = seperate_data(data)\n",
        "        self.test_data = None\n",
        "    \n",
        "    def train0(self):\n",
        "        self.dictionary = {}\n",
        "        self.numClasses = [0, 0, 0]\n",
        "        self.numWords = [0, 0, 0]\n",
        "        for row in self.train_data:\n",
        "            if row[2] == 'BUSINESS':\n",
        "                self.numClasses[0] += 1\n",
        "            elif row[2] == 'TRAVEL':\n",
        "                self.numClasses[1] += 1\n",
        "            else:\n",
        "                continue\n",
        "            \n",
        "            for word in row[6]:\n",
        "                if not word in self.dictionary:\n",
        "                    self.dictionary[word] = [0, 0, 0]\n",
        "                if row[2] == 'BUSINESS':\n",
        "                    self.dictionary[word][0] += 1\n",
        "                    self.numWords[0] += 1\n",
        "                elif row[2] == 'TRAVEL':\n",
        "                    self.dictionary[word][1] += 1\n",
        "                    self.numWords[1] += 1\n",
        "\n",
        "    def train(self):\n",
        "        self.dictionary = {}\n",
        "        self.numClasses = [0, 0, 0]\n",
        "        self.numWords = [0, 0, 0]\n",
        "        for row in self.train_data:\n",
        "            if row[2] == 'BUSINESS':\n",
        "                self.numClasses[0] += 1\n",
        "            elif row[2] == 'TRAVEL':\n",
        "                self.numClasses[1] += 1\n",
        "            elif row[2] == 'STYLE & BEAUTY':\n",
        "                self.numClasses[2] += 1\n",
        "            \n",
        "            for word in row[6]:\n",
        "                if not word in self.dictionary:\n",
        "                    self.dictionary[word] = [0, 0, 0]\n",
        "                if row[2] == 'BUSINESS':\n",
        "                    self.dictionary[word][0] += 1\n",
        "                    self.numWords[0] += 1\n",
        "                elif row[2] == 'TRAVEL':\n",
        "                    self.dictionary[word][1] += 1\n",
        "                    self.numWords[1] += 1\n",
        "                elif row[2] == 'STYLE & BEAUTY':\n",
        "                    self.dictionary[word][2] += 1\n",
        "                    self.numWords[2] += 1\n",
        "\n",
        "    def label(self, datan, phase):\n",
        "        if datan == \"train\":\n",
        "            data = self.train_data\n",
        "        elif datan == \"valid\":\n",
        "            data = self.valid_data\n",
        "        else:\n",
        "            data = self.test_data\n",
        "        \n",
        "        labels = []\n",
        "        for row in data:\n",
        "            score1 = log(1)\n",
        "            score2 = log(1)\n",
        "            score3 = log(1)\n",
        "            nameClasses = ['BUSINESS', 'TRAVEL', 'STYLE & BEAUTY']\n",
        "\n",
        "            if datan == \"test\":\n",
        "                sentence = row[4]\n",
        "            else:\n",
        "                sentence = row[6]\n",
        "\n",
        "            for word in sentence:\n",
        "                if not word in self.dictionary:\n",
        "                    continue\n",
        "                if self.dictionary[word][0] == 0:\n",
        "                    score1 -= 10\n",
        "                else:\n",
        "                    score1 += log((self.dictionary[word][0])/self.numWords[0])\n",
        "                if self.dictionary[word][1] == 0:\n",
        "                    score2 -= 10\n",
        "                else:\n",
        "                    score2 += log((self.dictionary[word][1])/self.numWords[1])\n",
        "                if phase == 1:\n",
        "                    if self.dictionary[word][2] == 0:\n",
        "                        score3 -= 10\n",
        "                    else:\n",
        "                        score3 += log((self.dictionary[word][2])/self.numWords[2])\n",
        "            \n",
        "            score1 += log((self.numClasses[0])/sum(self.numClasses))\n",
        "            score2 += log((self.numClasses[1])/sum(self.numClasses))\n",
        "            if phase == 1:\n",
        "                score3 += log((self.numClasses[2])/sum(self.numClasses))\n",
        "\n",
        "            if phase == 0:\n",
        "                tmp = [score1, score2]\n",
        "                labels.append(nameClasses[tmp.index(max(tmp))])\n",
        "            else:\n",
        "                tmp = [score1, score2, score3]\n",
        "                labels.append(nameClasses[tmp.index(max(tmp))])\n",
        "        return labels\n",
        "            \n",
        "    def scores(self, data, phase):\n",
        "        def count_correct_labels(truth_labels, label):\n",
        "            count = 0\n",
        "            for row in truth_labels:\n",
        "                if row[0] == row[1] and row[0] == label:\n",
        "                    count +=1\n",
        "            return count\n",
        "        \n",
        "        def count_detected_labels(truth_labels, label, phase):\n",
        "            count = 0\n",
        "            for row in truth_labels:\n",
        "                if phase==0 and row[0] == 'STYLE & BEAUTY':\n",
        "                    continue\n",
        "                if row[1] == label:\n",
        "                    count += 1\n",
        "            return count   \n",
        "        \n",
        "        def count_labels(truth_labels, label):\n",
        "            count = 0\n",
        "            for row in truth_labels:\n",
        "                if row[0] == label:\n",
        "                    count += 1\n",
        "            return count\n",
        "        \n",
        "        def count_trues(truth_labels, phase):\n",
        "            count = 0\n",
        "            all = 0\n",
        "            for row in truth_labels:\n",
        "                if phase == 0 and row[0] == 'STYLE & BEAUTY':\n",
        "                    continue\n",
        "                all += 1\n",
        "                if row[0] == row[1]:\n",
        "                    count += 1\n",
        "            return count, all\n",
        "        \n",
        "        labels = self.label(data, phase)\n",
        "\n",
        "        if data == \"train\":\n",
        "            data = self.train_data\n",
        "        elif data == \"valid\":\n",
        "            data = self.valid_data\n",
        "        else:\n",
        "            data = self.test_data\n",
        "        \n",
        "\n",
        "        recalls = {'BUSINESS':0, 'TRAVEL':0, 'STYLE & BEAUTY':0}\n",
        "        precisions = {'BUSINESS':0, 'TRAVEL':0, 'STYLE & BEAUTY':0}\n",
        "        accuracys = 0\n",
        "\n",
        "        truth_labels = []\n",
        "        for i in range(len(data)):\n",
        "            tmp = []\n",
        "            tmp.append(data[i][2])\n",
        "            tmp.append(labels[i])\n",
        "            truth_labels.append(tmp)\n",
        "        confusion = confusion_matrix(truth_labels)\n",
        "        print(\"confusion matrix \")\n",
        "        print(confusion)\n",
        "        recalls['BUSINESS'] = count_correct_labels(truth_labels, 'BUSINESS')/count_labels(truth_labels, 'BUSINESS')\n",
        "        precisions['BUSINESS'] = count_correct_labels(truth_labels, 'BUSINESS')/count_detected_labels(truth_labels, 'BUSINESS', phase)\n",
        "        recalls['TRAVEL'] = count_correct_labels(truth_labels, 'TRAVEL')/count_labels(truth_labels, 'TRAVEL')\n",
        "        precisions['TRAVEL'] = count_correct_labels(truth_labels, 'TRAVEL')/count_detected_labels(truth_labels, 'TRAVEL', phase)\n",
        "        \n",
        "        if phase==1:\n",
        "            recalls['STYLE & BEAUTY'] = count_correct_labels(truth_labels, 'STYLE & BEAUTY')/count_labels(truth_labels, 'STYLE & BEAUTY')\n",
        "            precisions['STYLE & BEAUTY'] = count_correct_labels(truth_labels, 'STYLE & BEAUTY')/count_detected_labels(truth_labels, 'STYLE & BEAUTY', phase)\n",
        "\n",
        "        trues, alll = count_trues(truth_labels, phase)\n",
        "        accuracys = trues/alll\n",
        "        return recalls, precisions, accuracys\n",
        "\n",
        "    def test(self, file_name):\n",
        "        data_fields, self.test_data = read_csv_test(file_name)\n",
        "        labels = self.label(\"test\", 1)\n",
        "        with open('output.csv', 'w', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow(['index', 'category'])\n",
        "            for i,row in enumerate(labels):\n",
        "                writer.writerow([i, row])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dVZdLZ6sxEc",
        "colab_type": "text"
      },
      "source": [
        "Result Tables\n",
        "<table>\n",
        "    <tr>\n",
        "        <th>Phase1</th>\n",
        "        <th>Travel</th>\n",
        "        <th>Business</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <th>Recall</th>\n",
        "        <td>0.80</td>\n",
        "        <td>0.90</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <th>Precision</th>\n",
        "        <td>0.89</td>\n",
        "        <td>0.82</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <th>Accuracy</th>\n",
        "        <td></td>\n",
        "        <td>0.85</td>\n",
        "    </tr>\n",
        "</table>\n",
        "</br>\n",
        "<table>\n",
        "    <tr>\n",
        "        <th>Phase2</th>\n",
        "        <th>Travel</th>\n",
        "        <th>Business</th>\n",
        "        <th>Style & Beauty</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <th>Recall</th>\n",
        "        <td>0.76</td>\n",
        "        <td>0.90</td>\n",
        "        <td>0.81</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <th>Precision</th>\n",
        "        <td>0.83</td>\n",
        "        <td>0.76</td>\n",
        "        <td>0.89</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <th>Accuracy</th>\n",
        "        <td></td>\n",
        "        <td>0.82</td>\n",
        "        <td></td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXJezMg_7tB1",
        "colab_type": "text"
      },
      "source": [
        "##What happens if we just have a high amount of precision?\n",
        "\n",
        "Suppose that you have system that diagnoses a disease and just 1 percent of people who are experimented are sick, so if your machine label every person as healthy, so your precision for healthy people would be 0.99 but your machine does nothing at all, so we cannot take the precision value as a measurement for evaluation of our machine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9SxeIgn_AO-",
        "colab_type": "text"
      },
      "source": [
        "##TF-IDF\n",
        "This is a measurment to identify the importance of a word to a document in a corpus or collection.\n",
        "\n",
        "Computation: TermFrequency * InverseDocumentFrequency\n",
        "\n",
        "TermFrequency: tf(t,d) = num of occurance of term t in document d/num of all words in document d\n",
        "\n",
        "InverseDocumentFrequency: idf(t,d)=log(num of occurance of the term t/num of the documents containing term t)\n",
        "\n",
        "the tf(t,d) specifies the how common a word is in the document, and the idf(t,d) specifies how unique or rare a word is among all the corpus\n",
        "\n",
        "So through this approach we would identify the most important words in every document and prune the ones that are totally much frequent and not important in the corpus.\n",
        "\n",
        "In order to use this method in our bayesian approach, I think we can multiplicate the second term of the tf-idf to the probability we compute for each word, and the term would be log(num of sentences have the word/num of classes have the word) and this term would be a measurement how the word is distributed along different classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h01GEV-YGFdG",
        "colab_type": "text"
      },
      "source": [
        "##What happens if you see a word you haven't seen in any other class?\n",
        "\n",
        "According to our method of computation, if the num of occurance of the word in a class is zero it means log of its probability would be -inf and it cannot be stored well, so we put -10 instead which causes a much level of decrease for the chance of the class to be chosen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtC7rKRQNfpL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "ff44729f-5e11-45e8-c3ea-b9c203ba2d06"
      },
      "source": [
        "model = Model(\"data.csv\")\n",
        "model.train()\n",
        "a, b, c = model.scores(\"valid\", 1)\n",
        "print(\"Recall\", a)\n",
        "print(\"Precision\", b)\n",
        "print(\"Accuracy\", c)\n",
        "model.test(\"test.csv\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "confusion matrix \n",
            "{'BUSINESS': {'BUSINESS': 1583, 'TRAVEL': 118, 'STYLE & BEAUTY': 77}, 'STYLE & BEAUTY': {'STYLE & BEAUTY': 1455, 'TRAVEL': 140, 'BUSINESS': 183}, 'TRAVEL': {'BUSINESS': 316, 'TRAVEL': 1334, 'STYLE & BEAUTY': 128}}\n",
            "Recall {'BUSINESS': 0.8903878583473862, 'TRAVEL': 0.7504215851602024, 'STYLE & BEAUTY': 0.8184373243395165}\n",
            "Precision {'BUSINESS': 0.7604416706673067, 'TRAVEL': 0.8380414312617702, 'STYLE & BEAUTY': 0.876580373269115}\n",
            "Accuracy 0.8197489226157018\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}