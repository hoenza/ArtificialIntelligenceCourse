{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI - Final - Computer Assignment\n",
    "## Hossein Entezari Zarch - 810196419\n",
    "## Summer - 2020\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from __future__ import unicode_literals\n",
    "import hazm\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "from tqdm import trange, tqdm\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Introduction\n",
    "<p dir='rtl'>\n",
    "    مقدمه: هدف ما در اینجا به طور کلی ساختن یک تابع(مدل پیش‌بینی) است که بتواند با استفاده از داده‌هایی که به عنوان دیتاست به ما داده شده است، به خوبی الگوهای موجود در داده‌ها را کشف کند و آنها را یاد بگیرد. این مدل توانایی خواهد داشت که با دیدن و یادگیری روی داده‌هایی که ما به آن می‌دهیم مثال‌هایی که تا به‌حال ندیده را پیش‌بینی و حل کند.\n",
    "    <br><br>\n",
    "    روند کلی: روند کلی ای که ما برای انجام این فرآیند پیش می‌گیریم شامل: ۱-پیش‌پردازش داده‌ها و استخراج ویژگی‌ها ۲-جدا کردن بخشی از داده ها به عنوان داده آزمایش ۳-تعریف مدل و انجام عملیات یادگیری روی آن ۴-پیش‌بینی داده‌های آزمایش و مقایسه با مقادیر واقعی هدف\n",
    "    <br><br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcess & Feature Extraction\n",
    "<p dir='rtl'>\n",
    "    به طور کلی در اینجا هدف ما تبدیل داده‌ها به شکلی است که برای مدل ما قابل فهم‌تر باشند و بیشتر مدل بتواند از آنها اطلاعات یاد بگیرد، به منظور این هدف ما فیلد‌های مختلف داده‌ها را به صورت های مختلف تحلیل کرده و ذخیره می‌کنیم و همچنین در این مرحله داده‌هایی مقادیر نامناسب(داده پرت) هستند را حذف می‌کنیم.\n",
    "    <br><br>\n",
    "    encodeing متغیر های گسسته:\n",
    "    به طور کلی در ساده ترین راه‌ها برای encode\n",
    "    کردن این نوع داده‌ها، دو روش Labeling, OneHotEncoding\n",
    "    استفاده م‌شود که به طور خلاصه در روش Labeling \n",
    "    هر مقدار از متغیر به یک عدد یکتا مپ می‌شود و آن عدد به جای آن مقدار ذخیره می‌شود و در روش OneHotEncoding\n",
    "    به ازای هر مقدار خاص از متغیر یک جایگاه اختصاص داده می‌شود که اگر آن مقدار رخ داده جایگاه متناظرش ۱ می‌شود و بقیه جایگاه‌ها صفر می‌مانند.\n",
    "    <br><br>\n",
    "    ما در اینجا در encode \n",
    "    کردن {برند، شهر} از روش OneHotEncoding \n",
    "    استفاده کردیم زیرا با توجه به این که از مدل MultiLayerPerceptronRegressor\n",
    "    استفاده می‌کنیم و خروجی در این مدل در اصل ترکیبی غیرخطی(وجود تابع غیرخطی بین لایه‌ها) از مقادیر ورودی است لذا مقادیر مختلف معنای فاصله بین‌شان تعریف نمی‌شود و اگر دو مقدار Index\n",
    "    متناظرشان نزدیک هم باشد یا یکی از دیگری بزرگ‌تر باشد معنایی ندارد ولی این می‌تواند شبکه را به اشتباه بیندازد ولی در روش استفاده شده ما همه مقادیر ارزش و اعتبار یکسان دارند و این تنها وزن‌های شبکه است که میان آنها تمایز ایجاد می‌کند. پش با این استدلالات فیلدهای {برند، شهر} که تعداد مقادیرشان نیز زیاد نیست با OneHotEncoding\n",
    "    انکود کردیم.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_brand(data):\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    enc_df = pd.DataFrame(enc.fit_transform(data[['brand']]).toarray())\n",
    "    enc_df.columns = enc.get_feature_names(['brand'])\n",
    "    data = data.join(enc_df)\n",
    "    data = data.drop('brand', axis=1)\n",
    "    return data\n",
    "\n",
    "def one_hot_encode_city(data):\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    enc_df = pd.DataFrame(enc.fit_transform(data[['city']]).toarray())\n",
    "    enc_df.columns = enc.get_feature_names(['city'])\n",
    "    data = data.join(enc_df)\n",
    "    data = data.drop('city', axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cycle Encode\n",
    "<p dir='rtl'>\n",
    "  در مورد فیلد {روز و ساعت}، به طور کلی ما از اهمیت این داده‌ها در فرآیند یادگیری و میزان تاثیر آنها در یادگیری مدل (میزان وابستگی قیمت به این فیلد) آگاه نیستیم ولی برای اطمینان و با توجه به این که مدل ما در صورت نیاز از این داده استفاده خواهد کرد و یا در غیر این صورت خیر، پس ابتدا باید داده‌ها را به صورتی که برای مدل به راحتی قابل فهم باشد در می‌آوریم  سپس آنها را در داده ذخیره کنیم.\n",
    "    <br><br>\n",
    "    Cyclic Encoding:\n",
    "    می‌دانیم که داده‌های مربوط به تاریخ و ساعت هم توالی دارند و هم تکرار یعنی به عنوان مثال شنبه بسیار به جمعه نزدیک است و این توالی ادامه دارد لذا بهترین راهی که به نظر من رسید، این بود که این داده‌ها را در تابع sin, cos\n",
    "    که توابعی متناوب هستند مدل کنیم. برای این مورد هر کدام از ساعت و روز را جداگانه با فاصله مساوی زاویه‌ای روی دایره مثلثاتی تعریف کردیم و sin, cos\n",
    "    متناظر آن زاویه را در نظر گرفتیم.\n",
    "    <br><br>\n",
    "    پیاده‌سازی: برای پیاده‌سازی این هدف، ابتدا لیست مقادیر متمایز این متغیر ها را تهیه کردیم و سپس هر کدام از آنها را به ترتیب و با فاصله یکسان روی دایره مثلثاتی قرار داده و لیست سینوس و کیسنوس متناظرشان را ساختیم و در نهایت با مراجه به آن لیست، مقادیر این فیلد را تبدیل و جایگزین فیلد اولیه کردیم.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_encode(values, value):\n",
    "    return [np.sin(2*np.pi*values.index(value)/len(values)), np.cos(2*np.pi*values.index(value)/len(values))]\n",
    "\n",
    "def make_week_day_cycle_list():\n",
    "    ans = {}\n",
    "    values = ['Saturday', 'Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\n",
    "    for value in values:\n",
    "        ans[value] = cycle_encode(values, value)\n",
    "    return ans\n",
    "\n",
    "def make_hour_cycle_list():\n",
    "    ans = {}\n",
    "    values = ['01AM', '02AM', '03AM', '04AM', '05AM', '06AM', '07AM', '08AM', '09AM', '10AM', '11AM', '12AM',\n",
    "              '01PM', '02PM', '03PM', '04PM', '05PM', '06PM', '07PM', '08PM', '09PM', '10PM', '11PM', '12PM']\n",
    "    for value in values:\n",
    "        ans[value] = cycle_encode(values, value)\n",
    "    return ans\n",
    "\n",
    "def created_at_encode(data):\n",
    "    day_sin = []\n",
    "    day_cos = []\n",
    "    sinc = make_week_day_cycle_list()\n",
    "    \n",
    "    hour_sin = []\n",
    "    hour_cos = []\n",
    "    hours = make_hour_cycle_list()\n",
    "    \n",
    "    for element in data['created_at']:\n",
    "        [day, hour] = element.split()\n",
    "        day_sin.append(sinc[day][0])\n",
    "        day_cos.append(sinc[day][1])\n",
    "        \n",
    "        hour_sin.append(hours[hour][0])\n",
    "        hour_cos.append(hours[hour][1])\n",
    "    new_df = pd.DataFrame({'day_sin': day_sin, 'day_cos': day_cos, 'hour_sin': hour_sin, 'hour_cos': hour_cos})\n",
    "    data = data.join(new_df)\n",
    "    data = data.drop('created_at', axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Process\n",
    "<p dir='rtl'>\n",
    "    به طور مشخص دو فیلد {عنوان، توضیحات} که تنها فیلدهایی هستند که مدل گوشی و وضعیت آن را مشخص می‌کنند قطعا نقش بسازایی در عملکرد شبکه دارند اما این فیلدها به این صورت خام قابل استفاده و ورودی دادن به شبکه نیستند لذا باید با روش مناسب اطلاعات مفیدی از آنها استخراج کنیم.\n",
    "    <br><br>\n",
    "    برای استخراج اطلاعات از این دو فیلد، ابتدا تمامی کلمات موجود در هر فیلد را جداگانه می‌خوانیم و تمامی کلمات به همراه تعداد تکرار شان را محاسبه می‌کنیم، سپس به دلیل تعداد زیاد کلمات و همچنین تعداد زیاد کلماتی که تکرار آنها بسیار پایین است، کلمه‌های کم‌تکرار را از لیست کلمات حذف می‌کنیم تا به این وسیله هم تعداد ویژگی ها کم شود و هم با حذف کلمات کم‌تکرار از overfitting\n",
    "    مدل جلوگیری کنیم (زیرا اگر مثلا کلمه‌ای که یک بار تنها تکرار شده را به عنوان ویژگی به شبکه بدهیم شبکه می‌تواند آن داده را عملا حفظ کند و هر وقت آن ویژگی را دید قیمت آن محصول را خروجی دهد و این کار ارزش یادگیری ندارد).\n",
    "    <br><br>\n",
    "    کار با متن: به دلیل این که با متنی مواجه هستیم که بسیار نامتوازن و حاوی فاصله‌های نامناسب است قبل از هر بار خواندن هر متن ابتدا آن را به وسیله hazm.Normalizer()\n",
    "    نرمالایز می‌کنیم تا کمی از نامتوازنی متن کاسته شود و سپس با استفاده از hazm.tokenize()\n",
    "    آرایه حاوی کلمات متن را دریافت می‌کنیم و سپس روی هر کلمه hazm.Stemmer()\n",
    "    اعمال می‌کنیم تا ریشه کلمه بدست آید و این ریشه کلمه معیار ما برای وقوع آن کلمه(ویژگی) در متن خواهد بود.\n",
    "    <br><br>\n",
    "    توضیح: علت معیار قرار دادن کلمات به عنوان ویژگی آن است که به طور کلی وقوع بعضی کلمات خاص می‌توانند معنای مثبت یا منفی در متن ایجاد کنند و البته که این روش خطا بسیار دارد ولی با توجه به امکانات ما در تحلیل متن و نوع متن نامتوازنی که در داده‌ها بود تصمیم گرفتم تا صرفا بر اساس وقوع کلمات ویژگی‌هایی را استخراج کنم.\n",
    "    <br><br>\n",
    "    در این جا کلماتی که وقوع آنها کمتر بوده از لیست کلی کلمات حذف شدند(برای عنوان، کلمات با تکرار بالای ۱۰۰ و برای توضیحات، کلمات با تکرار بالای ۵۰۰ استفاده شدند) و همچنن کلماتی که تنها یک کاراکتر داشتند نیز به طور کلی حذف شدند همچنین قطعا حذف کلمات پرتکراری که stop word\n",
    "    هستند و عملا هرجایی می‌توانند بیایند و معنای خاصی به مدل نمی‌دهند می‌توانست بسیار کمک کند تا مدل سبک‌تری ساخته شود اما با توجه به این که کتابخانه این ابزار را نداشت نتوانستم این کار را انجام دهم و البته حضور این کلمات تداخلی در امر یادگیری شبکه ایجاد نمی‌کند زیر شبکه می‌تواند کارکرد آنها یا عدم کارکرد آنها را درک کند.\n",
    "    <br><br>\n",
    "    حال پس از نهایی کردن این لیست های کلمات که یکی بر اساس متن title\n",
    "    و دیگری بر اساس متن desc\n",
    "    هستند، تقریبا در مجموع به ۴۹۰ کلمه رسیدیم که در این مرحله به هر داده یک بردار oneHot\n",
    "    به طول تعداد این کلمات اختصاص می‌دهیم تا وقوع این کلمات در هر یک از دو فیلد خود را ذخیره کند و این کار با استفاده از پیمایش روی داده ها انجام می‌شود.\n",
    "    <br><br>\n",
    "    به طور کلی علت این که متن های موجود در دو فیلد را جداگانه تحلیل و بررسی کردم این بود که به نظرم آمد دامنه کلمات این دو فیلد می‌تواند متفاوت باشد و وقوع کلمه در هر یک ممکن است بتواند معنای متفاوتی ایجاد کند.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_library(data):\n",
    "    normalizer = hazm.Normalizer()\n",
    "    stemmer = hazm.Stemmer()\n",
    "    \n",
    "    title_wl = {}\n",
    "    for sentence in data['title']:\n",
    "        normal_sentence = normalizer.normalize(sentence)\n",
    "        words = hazm.word_tokenize(normal_sentence)\n",
    "        for word in words:\n",
    "            sw = stemmer.stem(word)\n",
    "            if sw not in title_wl:\n",
    "                title_wl[sw] = 0\n",
    "            title_wl[sw] += 1\n",
    "    title_wl = {key:val for key, val in title_wl.items() if val > 100 and len(key) > 1}\n",
    "    title_wl = {k: v for k, v in sorted(title_wl.items(), key=lambda item: item[1])}\n",
    "    \n",
    "    desc_wl = {}\n",
    "    for sentence in data['desc']:\n",
    "        normal_sentence = normalizer.normalize(sentence)\n",
    "        words = hazm.word_tokenize(normal_sentence)\n",
    "        for word in words:\n",
    "            sw = stemmer.stem(word)\n",
    "            if sw not in desc_wl:\n",
    "                desc_wl[sw] = 0\n",
    "            desc_wl[sw] += 1\n",
    "    desc_wl = {key:val for key, val in desc_wl.items() if val > 500 and len(key) > 1}\n",
    "    desc_wl = {k: v for k, v in sorted(desc_wl.items(), key=lambda item: item[1])}\n",
    "                \n",
    "    return title_wl.keys(), desc_wl.keys()\n",
    "\n",
    "def encode_text(data):\n",
    "    normalizer = hazm.Normalizer()\n",
    "    stemmer = hazm.Stemmer()\n",
    "    \n",
    "    title_words, desc_words = generate_library(data)\n",
    "    title_data = {}\n",
    "    desc_data = {}\n",
    "    for word in title_words:\n",
    "        title_data['title_'+word] = [0 for _ in data['title']]\n",
    "    for (i,sentence) in enumerate(data['title']):\n",
    "        normal_sentence = normalizer.normalize(sentence)\n",
    "        words = hazm.word_tokenize(normal_sentence)\n",
    "        for word in words:\n",
    "            sw = stemmer.stem(word)\n",
    "            if 'title_'+sw in title_data:\n",
    "                title_data['title_'+sw][i] = 1\n",
    "    \n",
    "    for word in desc_words:\n",
    "        desc_data['desc_'+word] = [0 for _ in data['desc']]\n",
    "    for (i, sentence) in enumerate(data['desc']):\n",
    "        normal_sentence = normalizer.normalize(sentence)\n",
    "        words = hazm.word_tokenize(normal_sentence)\n",
    "        for word in words:\n",
    "            sw = stemmer.stem(word)\n",
    "            if 'desc_'+sw in desc_data:\n",
    "                desc_data['desc_'+sw][i] = 1\n",
    "\n",
    "    \n",
    "    title_data = pd.DataFrame(title_data)\n",
    "    desc_data = pd.DataFrame(desc_data)\n",
    "    data = data.join(title_data)\n",
    "    data = data.join(desc_data)\n",
    "    data = data.drop(['title', 'desc'] , axis=1).dropna()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Normalization\n",
    "<p dir='rtl'>\n",
    "    به طور کلی برای آموزش هر مدلی که بر پایه شبکه عصبی است باید داده‌ها را نرمالایز کنیم، که در این فرایند داده‌های هر فیلد به مقادیر بین {-۱و ۱} اسکیل می‌شوند به طوری که میانگین شان ۰ و واریانس شان ۱ شود و این عمل باعث می‌شود مدل روی این داده‌ها خوش فرم‌تر باشد و لذا بسیار زودتر و بهینه‌تر مدل همگرا شود، همچنین به  تحربه دیده شده که در بسیاری از مدل‌های شبکه عصبی که داده‌های یادگیری شان نرمال نشده شبکه هیچ چیزی یاد نگرفته و هیچ بهبودی در عملکردش نداشته است.\n",
    "    <br><br>\n",
    "    ما این عملیات نرمال‌سازی را پس از اتمام تمامی مراحل پیش‌پردازش روی داده‌ها، روی تمامی داده‌ها و تمامی فیلد‌های آنها انجام دادیم تا از این طریق اطمینان یابیم که روی تمامی داده‌هایمان دقیقا یک عملیات اسکیل انجام گرفته است. همچنین این تابع که نرمالایز کردن داده‌ها را انجام می‌دهد مدل scaler\n",
    "    را نیز خروجی می‌دهد تا بتوانیم برای ورودی دادن داده‌هایی که قبلا نرمال نشده‌اند به شبکه ترین شده، ابتدا آنها را با گذراندن از این مدل نرمال کنیم.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data):\n",
    "    columns = data.columns\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(data)\n",
    "    x = scaler.transform(data)\n",
    "    x = pd.DataFrame(x, columns=columns)\n",
    "    return x, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data & Perfrom PreProcess\n",
    "<p dir='rtl'>\n",
    "    در این مرحله داده‌ها را از فایل می‌خوانیم و توابعی که بالاتر تعریف کرده بودیم را روی آنها صدا می‌زنیم تا عملیات آماده‌سازی داده‌ها انجام گیرد. همچنین در این مرحله داده‌هایی که قیمت شان زیر ۱۰۰۰۰ را حذف می‌کنیم زیرا قیمت به این پایینی منطقی نیست و احتمالا یا داده قیمت ندارد و مقدارش -۱ است یا قیمت اشتباه وارد شده است.\n",
    "    <br><br>\n",
    "    علاوه بر این موارد، داده‌هایی که قیمت‌شان -۱ است را نیز جدا کردیم و در دیتافریم data_non_price \n",
    "    ذخیره کردیم.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('mobile_phone_dataset.csv').drop(['Unnamed: 0'], axis=1).dropna().reset_index()\n",
    "data = one_hot_encode_brand(data)\n",
    "data = one_hot_encode_city(data)\n",
    "data = created_at_encode(data)\n",
    "data = encode_text(data)\n",
    "data_non_price = data[data['price'] == -1].reset_index()\n",
    "data = data[data['price'] > 10000].reset_index().dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Data Normalization\n",
    "<p dir='rtl'>\n",
    "در این قسمت ستون قیمت از داده‌را به عنوان هدف(y\n",
    ")\n",
    "تعیین می‌کنیم وبرای ویژگی‌های داده‌ها نیز ستون قیمت را حذف کرده و بقیه را به عنوان\n",
    "X\n",
    "تعیین می‌کنیم.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, data_scaler = normalize_data(data)\n",
    "y = data['price']\n",
    "X = data.drop('price', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test Set\n",
    "<p dir='rtl'>\n",
    "مطابق همیشه باید داده‌ها به دو بخش یادگیری و آزمایش تقسیم شوند تا بخشی از داده ها موجود باشند که مدل طی فرایند یادگیری آن‌را نمی‌بیند و از این داده‌های جدید برای مدل به منظور امتحان کردن و تست کارکرد مدل و محاسبه خطا استفاده می‌شود.\n",
    "    <br><br>\n",
    "    در اینجا ما ۲۰ درصد از داده‌ها را به عنوان داده آزمایش جدا می‌کنیم و در زمان بررسی عملکرد مدل از آن داده‌ها استفاده می‌کنیم. که اینجا تابع train_test_split()\n",
    "    از کتابخانه sklearn\n",
    "    این کار را برای ما انجام می‌دهد.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "<p dir='rtl'>\n",
    "    در اینجا ما مدل MLPRegressor()\n",
    "    را به عنوان مدلی که بتواند این داده‌ها را تعداد شان تقریبا ۴۰۰۰۰ تا و تعداد ویژگی‌هایشان ۵۰۰ تا است را یاد بگیرد انتخاب کردیم.\n",
    "    <br><br>\n",
    "    این مدل در اصل یک شبکه چندلایه MLP\n",
    "    فولی‌کانکتد است که این شبکه‌ها توانایی زیادی در تشخیص الگوهای موجود در داده‌ها دارند زیرا در چند لایه ما به متغیر های با درجات مختلف بر حسب تمامی وروردی‌ها می‌رسیم و این توانایی زیادی به شبکه می‌دهد.\n",
    "    <br><br>\n",
    "    ابعاد مدل: ما برای لایه‌های پنهان این مدل ابعاد ۱۰۰۰و۲۰۰۰و۱۰ را انتخاب کردیم تا در ابتدا به ویژگی‌های کلی از تمامی ویژگی‌ها برسد و سپس در لایه‌های بعدی به ویژگی قیمت که هدف ماست برسد. برای تابع غیرخطی بین لایه‌ها نیز شبکه به صورت پیش‌فرض از relu\n",
    "    استفاده می‌کند که از مزایای این تابع می‌توان به موارد زیر اشاره کرد.\n",
    "    <br><br>\n",
    "    ۱-\n",
    "    با توجه به این که مقدار تابع به ازای ورودی‌های منفی ۰ است، باعث ایجاد اسپارسیتی و ایجاد مقادیر بیشتری از صفر در شبکه و در نتیجه کاهش حجم محاسبات می‌شود. ۲- با توجه به اینکه در ناحیه مثبت شیب خط آن ثابت است و مانند sigmoid, tanh\n",
    "   با افزایش مقدار ورودی شیب خط آن به صفر میل نمی‌کند و این باعث می‌شود شبکه vanishing gradient\n",
    "    نکند و وزن‌های آن به صفر میل نکرده و سرعت یادگیری صفر نشود.\n",
    "    <br><br>\n",
    "    Adam Optimizer:\n",
    "    مدل ما برای یادگیری از ابزار بهینه‌سازی adam\n",
    "    استفاده می‌کند که روش کار آن بسیار شبیه به Stochastic Gradient Descent \n",
    "    است با این تفاوت های عمده که : ۱- برای هر وزن به طور جداگانه یک learning rate\n",
    "    اختصاص داده و آن را استفاده می‌کند که این مورد برای یادگیری شبکه‌های با گرادیان‌های اسپارس بسیار مناسب عمل می‌کند و ما در اینجا با توجه به بردارهای ontHot\n",
    "    که در ویژگی‌های داده‌ها بسیار استفاده کردیم شبکه و داده اسپارسی داریم و تعداد زیادی از ورودی‌های اولیه ما صفر هستند. ۲-از مومنت‌های مرتبه اول و دوم گرادیان‌ها برای تعیین این learning_rate \n",
    "    برای آپدیت وزن‌ها استفاده می‌کند.\n",
    "    <br><br>\n",
    "    در نهایت داده‌های ویژگی‌ها و هدف را به مدل می‌دهیم تا مدل روی آنها fit\n",
    "    را انجام دهد و مدل ترین شود.\n",
    "    <br><br>\n",
    "    این مدل به طور کلی به صورت پیش‌فرض عملیات یادگیری را ادامه می‌دهد تا زمانی که برای ۱۰ بار متوالی مقدار هزینه روی داده‌ها تغییر نکند و همچنین در حالت ماکزیمم این تعداد مراحل تکرار یادگیری می‌تواند به ۲۰۰ برسد. ما پس از یادگیری مدل با این شرایط و مقایسه مقدار هزینه روی داده‌های یادگیری و آموزش به اختلاف بسیار زیاد بین این مقادیر پی بردیم لذا ما تعداد ماکزیمم تکرار ها را به ۱۰ کاهش دادیم که از این طیق میزان آن اختلاف کاهش یافت اما همچنان تفاوت معناداری موجود است که می‌تواند نشان‌دهنده overfitting\n",
    "    باشد البته نباید از این نکته غافل شد که مدل پس از یادگیری همچنان دقت قایل قبولی روی داده‌های تست ارائه می‌دهد.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ho/ai/env/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(hidden_layer_sizes=(1000, 200, 10), max_iter=10)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLPRegressor(hidden_layer_sizes=(1000, 200, 10), max_iter=10)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Prediction\n",
    "<p dir='rtl'>\n",
    "    در اینحا تعداد تکرار های یادگیری مدل و هزینه مدل ترین شده روی داده‌های یادگیری و آزمایش را جداگانه بدست آوردیم و چاپ کردیم تا بتوان در مورد overfitting\n",
    "    نظر داد.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of iterations: 10\n",
      "train loss:  0.041991138293042636\n",
      " test loss:  0.29719289785080355\n"
     ]
    }
   ],
   "source": [
    "print('num of iterations:', model.n_iter_)\n",
    "y_pred = model.predict(X_train)\n",
    "train_loss = MSE(y_train, y_pred)\n",
    "print('train loss: ', train_loss)\n",
    "y_pred = model.predict(X_test)\n",
    "test_loss = MSE(y_test, y_pred)\n",
    "print(' test loss: ', test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate RMSE, MAE\n",
    "<p dir='rtl'>\n",
    "    در اینجا یک تابع زدیم تا قیمت‌های پیش‌بینی شده مدل را برای برگرداند به قبل از اسکیل شدن و عملیات عکس نرمالیزیشن را برای ما انجام دهد. سپس قیمت های پیش‌بینی رو داده‌های آزمایش را با قیت‌های واقعی‌شان مقایسه کردیم.\n",
    "    <br><br>\n",
    "    ما معیارهای RMSE, MAE\n",
    "    که به ترتیب جزر میانگین مربعات اختلاف داده‌ها و میانگین قدرمطلق اختلاف داده‌ها است را معیار این تفاوت قرار دادیم که به شرح زیر بدست آمدند.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse:  299516.1926484881\n",
      "mae:  199951.39069938275\n"
     ]
    }
   ],
   "source": [
    "def inverse_normalize_price(y):\n",
    "    dummy = pd.DataFrame(np.zeros((len(y), data_scaler.n_features_in_)), columns=data.columns)\n",
    "    dummy['price'] = y\n",
    "    dummy = pd.DataFrame(data_scaler.inverse_transform(dummy), columns=data.columns)\n",
    "    return dummy['price'].to_list()\n",
    "y_test = y_test.to_list()\n",
    "price_test = inverse_normalize_price(y_test)\n",
    "price_pred = inverse_normalize_price(y_pred)\n",
    "\n",
    "rmse = math.sqrt(MSE(price_test, price_pred))\n",
    "print('rmse: ', rmse)\n",
    "mae = MAE(price_test, price_pred)\n",
    "print('mae: ', mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Price for not having price data entries\n",
    "<p dir='rtl'>\n",
    "    در اینجا طبق خواسته سوال داده‌هایی که قیمت آنها -۱ بودند و تعیین شده بود را پیش‌بینی کردیم به این صورت که ابتدا آن داده‌ها را با نرمالایزر خود که روی داده‌ها فیت شده است نرمال کردیم و سپس قیمت های آن‌ها را با مدل پیش‌بینی کردیم و این قیمت‌ها را عکس نرمالایز کردی تا به قیمت‌های واقعی برسیم و آنها را در داده ‌های اولیه قرار دادیم و نتیجه را چاپ کردیم\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      level_0  index  image_count         price  brand_Apple::اپل  \\\n",
      "0          23     23            1  4.700436e+05               0.0   \n",
      "1          28     28            2  8.481692e+05               1.0   \n",
      "2          32     32            0  9.895222e+05               1.0   \n",
      "3          36     36            2  3.608371e+04               0.0   \n",
      "4          44     44            0  4.363417e+05               0.0   \n",
      "...       ...    ...          ...           ...               ...   \n",
      "5883    59121  59121            3  2.698836e+05               0.0   \n",
      "5884    59146  59146            3  1.494326e+05               0.0   \n",
      "5885    59169  59169            4  1.617941e+06               1.0   \n",
      "5886    59174  59174            0  1.433929e+06               0.0   \n",
      "5887    59183  59183            0  4.461220e+05               0.0   \n",
      "\n",
      "      brand_HTC::اچ‌تی‌سی  brand_Huawei::هوآوی  brand_LG::ال‌جی  \\\n",
      "0                     0.0                  0.0              0.0   \n",
      "1                     0.0                  0.0              0.0   \n",
      "2                     0.0                  0.0              0.0   \n",
      "3                     0.0                  0.0              0.0   \n",
      "4                     1.0                  0.0              0.0   \n",
      "...                   ...                  ...              ...   \n",
      "5883                  0.0                  0.0              0.0   \n",
      "5884                  0.0                  0.0              0.0   \n",
      "5885                  0.0                  0.0              0.0   \n",
      "5886                  0.0                  0.0              0.0   \n",
      "5887                  0.0                  1.0              0.0   \n",
      "\n",
      "      brand_Lenovo::لنوو  brand_Nokia::نوکیا  ...  desc_نو  desc_در  desc_خط  \\\n",
      "0                    0.0                 0.0  ...        1        1        0   \n",
      "1                    0.0                 0.0  ...        0        1        0   \n",
      "2                    0.0                 0.0  ...        0        0        0   \n",
      "3                    0.0                 0.0  ...        0        0        0   \n",
      "4                    0.0                 0.0  ...        0        1        0   \n",
      "...                  ...                 ...  ...      ...      ...      ...   \n",
      "5883                 1.0                 0.0  ...        1        0        0   \n",
      "5884                 0.0                 0.0  ...        1        1        0   \n",
      "5885                 0.0                 0.0  ...        1        1        1   \n",
      "5886                 0.0                 0.0  ...        0        1        0   \n",
      "5887                 0.0                 0.0  ...        0        0        0   \n",
      "\n",
      "      desc_به  desc_فقط  desc_تمیز  desc_بدون  desc_سال  desc_با  desc_گوش  \n",
      "0           0         0          0          0         0        1         1  \n",
      "1           0         0          1          0         0        0         1  \n",
      "2           0         0          0          1         0        0         0  \n",
      "3           0         0          1          1         0        0         1  \n",
      "4           0         0          0          1         1        1         1  \n",
      "...       ...       ...        ...        ...       ...      ...       ...  \n",
      "5883        0         1          0          0         0        0         1  \n",
      "5884        0         0          1          0         0        0         1  \n",
      "5885        1         0          1          1         0        0         1  \n",
      "5886        0         1          1          0         0        1         0  \n",
      "5887        0         0          0          0         0        0         0  \n",
      "\n",
      "[5888 rows x 564 columns]\n"
     ]
    }
   ],
   "source": [
    "non_normalized_data = data_scaler.transform(data_non_price)\n",
    "non_normalized_data = pd.DataFrame(non_normalized_data, columns=data_non_price.columns)\n",
    "non_prices = model.predict(non_normalized_data.drop('price', axis=1))\n",
    "non_pred_prices = inverse_normalize_price(non_prices)\n",
    "data_non_price['price'] = non_pred_prices\n",
    "print(data_non_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Implementation\n",
    "<p dir='rtl'>\n",
    "با توجه به این که با  ابزارهای موجود در اسکالرن قادر به تحلیل وضعیت overfitting\n",
    "به خوبی نبودیم و با وجودی که به دقت قابل قبولی روی داده‌های تست رسیده‌ایم اما برای تحلیل قصد پیاده‌سازی مدل با استفاده از پایتورچ را داریم.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, train_subset=0.8):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.columns = data.columns\n",
    "        self.train_subset = train_subset\n",
    "        self.X = data.drop('price', axis=1).to_numpy()\n",
    "        self.y = data['price'].to_numpy()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx]).float(), torch.tensor(self.y[idx]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward_net(nn.Module):\n",
    "    def __init__(self, network_dims, activation=None):\n",
    "        super().__init__()\n",
    "        self.dims = network_dims\n",
    "        self.activation = activation\n",
    "        self.layers = nn.ModuleList([nn.Linear(self.dims[i], self.dims[i+1]) for i in range(len(self.dims)-2)])\n",
    "        self.last_fc = nn.Linear(self.dims[-2], self.dims[-1])\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "            if self.activation == None:\n",
    "                X = F.relu(X)\n",
    "            elif self.activation == \"leakyRelu\":\n",
    "                X = F.leaky_relu(X)\n",
    "            elif self.activation == \"tanh\":\n",
    "                X = torch.tanh(X)\n",
    "#         X = self.dropout(X)\n",
    "        X = self.last_fc(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "dataset = MyDataset(data)\n",
    "train_set, test_set = random_split(dataset, [len(data)-int(len(dataset)*0.2), int(len(dataset)*0.2)])\n",
    "train_data_loader = DataLoader(train_set, batch_size=batch_size, sampler=SubsetRandomSampler(range(len(train_set))), drop_last=True)\n",
    "test_data_loader = DataLoader(test_set, batch_size=batch_size, drop_last=False)\n",
    "\n",
    "n_epochs = 10\n",
    "learning_rate = 0.01\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, test_loader, device=device, criterion=criterion, n_epoch=10):\n",
    "    total_time = 0  \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    for epoch in range(n_epoch):\n",
    "        epoch_loss = 0\n",
    "        d1 = datetime.now()\n",
    "        model.train()\n",
    "        for (batch, target) in tqdm(train_loader):\n",
    "            batch = batch.to(device)\n",
    "            target = target.to(device)\n",
    "            predicts = model(batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(predicts, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()  \n",
    "        \n",
    "        average_loss = epoch_loss/len(train_loader)\n",
    "        train_losses.append(average_loss)\n",
    "        d2 = datetime.now()\n",
    "        delta = d2 - d1\n",
    "        seconds = float(delta.total_seconds())\n",
    "        total_time += seconds\n",
    "        print('epoch %d, train_loss: %.3f, time elapsed: %s seconds' % (epoch + 1, average_loss, seconds))\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            epoch_loss = 0\n",
    "            for (batch, target) in test_loader:\n",
    "                batch = batch.to(device)\n",
    "                target = target.to(device)\n",
    "                predicts = model(batch)\n",
    "                loss = criterion(predicts, target)\n",
    "                epoch_loss += loss.item()\n",
    "            average_loss = epoch_loss/len(test_loader)\n",
    "            test_losses.append(average_loss)\n",
    "            print('test_loss: %.3f'%(average_loss))\n",
    "            \n",
    "    print('total training time: %.3f minutes' % (total_time / 60))\n",
    "    plt.plot(test_losses, label='test', color='r')\n",
    "    plt.plot(train_losses, label='train', color='b')\n",
    "    plt.show()\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-139-9474f5e05fac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeedforward_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ai/env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ai/env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ai/env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ai/env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ai/env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconvert_to_format\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure"
     ]
    }
   ],
   "source": [
    "dmodel = Feedforward_net([512, 1000, 200, 1]).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = Adam(dmodel.parameters(), lr=0.001)\n",
    "losses = train(dmodel, optimizer, train_data_loader, test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.loss.MSELoss'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'torch.Tensor'> <class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-fbcd60d46db3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprice_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minverse_normalize_price\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mprice_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minverse_normalize_price\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mmean_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprice_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprice_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-fbcd60d46db3>\u001b[0m in \u001b[0;36minverse_normalize_price\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minverse_normalize_price\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdummy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdummy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mdummy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdummy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ai/env/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2937\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2938\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2940\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ai/env/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2999\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3000\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3001\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ai/env/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   3639\u001b[0m                     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_convert_platform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3640\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3641\u001b[0;31m                     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray_tuplesafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3642\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3643\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ai/env/lib/python3.8/site-packages/pandas/core/common.py\u001b[0m in \u001b[0;36masarray_tuplesafe\u001b[0;34m(values, dtype)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconstruct_1d_object_array_from_listlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ai/env/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ai/env/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "y_pred = dmodel(torch.tensor(X_test.to_numpy()).float().to(device))\n",
    "print(type(criterion))\n",
    "print(type(y_pred))\n",
    "print(type(y_test))\n",
    "# loss = criterion(y_pred, y_test.to_numpy())\n",
    "# print('loss = ', loss)\n",
    "def inverse_normalize_price(y):\n",
    "    dummy = pd.DataFrame(np.zeros((len(y), data_scaler.n_features_in_)), columns=data.columns)\n",
    "    dummy['price'] = y\n",
    "    dummy = pd.DataFrame(data_scaler.inverse_transform(dummy), columns=data.columns)\n",
    "    return dummy['price'].to_list()\n",
    "# y_test = y_test.to_list()\n",
    "print(type(y_pred), type(y_test))\n",
    "price_test = inverse_normalize_price(y_test)\n",
    "price_pred = inverse_normalize_price(y_pred)\n",
    "\n",
    "mean_error = math.sqrt(MSE(price_test, price_pred))\n",
    "print('mean error = ', mean_error)\n",
    "for i in range(len(price_test)):\n",
    "    print(price_test[i], price_pred[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
